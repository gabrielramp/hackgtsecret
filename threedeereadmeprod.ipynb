{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "8-UmwuuA_ShY",
        "outputId": "a571a91e-b757-489a-89d9-d3a57fef44a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "creating base model...\n",
            "creating upsample model...\n",
            "downloading base checkpoint...\n",
            "downloading upsampler checkpoint...\n"
          ]
        }
      ],
      "source": [
        "#Initial Setup Cell\n",
        "!pip install git+https://github.com/openai/point-e -q\n",
        "!pip install pandas -q\n",
        "!pip install open3d -q\n",
        "!pip install numpy-stl -q\n",
        "!pip install trimesh -q\n",
        "!pip install flask -q\n",
        "!pip install pyngrok -q\n",
        "\n",
        "import trimesh\n",
        "from stl import mesh, stl\n",
        "import pandas as pd\n",
        "import open3d as o3d\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import threading\n",
        "import open3d as o3d\n",
        "import requests\n",
        "import base64\n",
        "from flask import Flask, request, redirect\n",
        "from pyngrok import ngrok\n",
        "from pyngrok import conf\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from point_e.util.pc_to_mesh import marching_cubes_mesh\n",
        "from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
        "from point_e.diffusion.sampler import PointCloudSampler\n",
        "from point_e.models.download import load_checkpoint\n",
        "from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
        "from point_e.util.plotting import plot_point_cloud\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('creating base model...')\n",
        "base_name = 'base40M-textvec'\n",
        "base_model = model_from_config(MODEL_CONFIGS[base_name], device)\n",
        "base_model.eval()\n",
        "base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
        "\n",
        "print('creating upsample model...')\n",
        "upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], device)\n",
        "upsampler_model.eval()\n",
        "upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
        "\n",
        "print('downloading base checkpoint...')\n",
        "base_model.load_state_dict(load_checkpoint(base_name, device))\n",
        "\n",
        "print('downloading upsampler checkpoint...')\n",
        "upsampler_model.load_state_dict(load_checkpoint('upsample', device))\n",
        "\n",
        "sampler = PointCloudSampler(\n",
        "      device=device,\n",
        "      models=[base_model, upsampler_model],\n",
        "      diffusions=[base_diffusion, upsampler_diffusion],\n",
        "      num_points=[1024, 4096-1024],\n",
        "      aux_channels=['R', 'G', 'B'],\n",
        "      guidance_scale=[3.0, 0.0],\n",
        "      model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-yVspiD_ShZ"
      },
      "outputs": [],
      "source": [
        "import hmac\n",
        "import hashlib\n",
        "from flask import jsonify\n",
        "\n",
        "GITHUB_WEBHOOK_SECRET = 'yellowclaw'\n",
        "\n",
        "# Global server flag\n",
        "server_busy = False\n",
        "\n",
        "# Constants\n",
        "TOKEN = 'TOKEN'\n",
        "OWNER = 'gabrielramp'\n",
        "REPO = 'hackgtsecret'\n",
        "BRANCH = 'main'\n",
        "FILE_PATH = 'README.md'\n",
        "COMMIT_MESSAGE = 'Update README.md with pizzaz!'\n",
        "HEADERS = {\n",
        "    'Authorization': f'token {TOKEN}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "\n",
        "def generateReadmeModel(prompt: str):\n",
        "    # Produce the model.\n",
        "    samples = None\n",
        "    for x in tqdm(sampler.sample_batch_progressive(batch_size=1, model_kwargs=dict(texts=[prompt]))):\n",
        "        samples = x\n",
        "    pc = sampler.output_to_point_clouds(samples)[0]\n",
        "    pc.save('pointcloud.npz')\n",
        "\n",
        "    # Convert the point cloud into a mesh.\n",
        "    print('creating SDF model...')\n",
        "    name = 'sdf'\n",
        "    model = model_from_config(MODEL_CONFIGS[name], device)\n",
        "    model.eval()\n",
        "\n",
        "    print('loading SDF model...')\n",
        "    model.load_state_dict(load_checkpoint(name, device))\n",
        "\n",
        "    # Produce a mesh\n",
        "    mesh = marching_cubes_mesh(\n",
        "        pc=pc,\n",
        "        model=model,\n",
        "        batch_size=4096,\n",
        "        grid_size=128,\n",
        "        progress=True,\n",
        "    )\n",
        "\n",
        "    with open('mesh.ply', 'wb') as f:\n",
        "        mesh.write_ply(f)\n",
        "\n",
        "    # Convert and simplify the mesh\n",
        "    o3d_mesh = o3d.io.read_triangle_mesh('mesh.ply')\n",
        "    decimation_target_number_of_triangles = int(0.01 * len(o3d_mesh.triangles))\n",
        "    simplified_mesh = o3d_mesh.simplify_quadric_decimation(decimation_target_number_of_triangles)\n",
        "    simplified_mesh.compute_vertex_normals()\n",
        "    o3d.io.write_triangle_mesh('simplified_output_mesh.stl', simplified_mesh)\n",
        "    print('done')\n",
        "\n",
        "    # Convert to ASCII STL\n",
        "    simplified_mesh_trimesh = trimesh.load('simplified_output_mesh.stl')\n",
        "    simplified_mesh_trimesh.export(file_obj='ascii_simplified_output_mesh.stl', file_type='stl_ascii')\n",
        "    print('done')\n",
        "\n",
        "    # Update the README\n",
        "    with open('ascii_simplified_output_mesh.stl', 'r') as f:\n",
        "        stl_content = f.read()\n",
        "\n",
        "    content = f\"Prompt: {prompt} \\n```stl\\n{stl_content}\\n``` \\n# 3DREADME \\nIs a text-to-model generative AI tool built entirely within GitHub! 3DREADME will take any prompt and create a 3D model for the described object using point-cloud to ASCII STL conversion. \\n\\n## How do I use it?\\nCreate a new issue in this repository through this link:\\n\\nhttps://github.com/gabrielramp/hackgtsecret/issues/new\\n\\nTitle the issue the text prompt you wish to be created into a model and displayed on this README.md. No body text is required in the new issue. You will automatically receive a response when the request is received, and the issue will be closed once this README is update with your model!\\n\\nThe model generator works best with simple objects, i.e 'a motorcycle', 'an airplane', or 'a laptop'.\\n\\n## How does it work?\\n\\n3DREADME first leverages GitHub's Webhooks and Issues to receive a text prompt from the user (you!). We then utilize a tweaked version OpenAI's Point-E model to generate a point-cloud field that represents the described object.\\n\\n![image](https://github.com/gabrielramp/hackgtsecret/assets/86631042/9f50ac08-2ce5-4111-8638-524ad337f219)\\n\\nThen, we estimate the normals of the verticies in the point-cloud by assessing their proximity to each other, and utilize the Ball Pivoting Algorithm to create faces in the mesh that results in a solid object.\\n\\n![image](https://github.com/gabrielramp/hackgtsecret/assets/86631042/a9f85174-6703-4c99-8c4c-393c76e815df)\\n(Image provided by IEEE.)\\n\\nAfter utilizing GitHub Webhooks to listen for Issue events, and the GitHub API to reply, update the README, and close your issue, the result is a water-tight representation of your text description displayed right in the README of the repository thanks to GitHub Diagrams!\"\n",
        "    update_readme(content)\n",
        "\n",
        "def update_readme(content):\n",
        "    try:\n",
        "        response = requests.get(f'https://api.github.com/repos/{OWNER}/{REPO}/contents/{FILE_PATH}', headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        sha = response.json()['sha']\n",
        "        encoded_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
        "        update_data = {\n",
        "            'message': COMMIT_MESSAGE,\n",
        "            'content': encoded_content,\n",
        "            'sha': sha,\n",
        "            'branch': BRANCH\n",
        "        }\n",
        "        response = requests.put(f'https://api.github.com/repos/{OWNER}/{REPO}/contents/{FILE_PATH}', json=update_data, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        print('README.md updated successfully!')\n",
        "    except requests.RequestException as error:\n",
        "        print(f'Error updating the README.md: {error}')\n",
        "\n",
        "def is_valid_signature(payload, signature):\n",
        "    secret_token = GITHUB_WEBHOOK_SECRET.encode()\n",
        "    computed_hmac = hmac.new(secret_token, payload, hashlib.sha1)\n",
        "    expected_signature = f\"sha1={computed_hmac.hexdigest()}\"\n",
        "    return hmac.compare_digest(expected_signature, signature)\n",
        "\n",
        "def reply_to_issue(issue_number, message):\n",
        "    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/issues/{issue_number}/comments\"\n",
        "    data = {\n",
        "        'body': message\n",
        "    }\n",
        "    response = requests.post(url, json=data, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "\n",
        "def close_issue(issue_number):\n",
        "    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/issues/{issue_number}\"\n",
        "    data = {\n",
        "        'state': 'closed'\n",
        "    }\n",
        "    response = requests.patch(url, json=data, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "\n",
        "# Spin up the Ngrok server\n",
        "conf.get_default().auth_token = \"TOKEN\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port, domain=\"threedeereadme.ngrok.dev\").public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, port))\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# Define Flask routes\n",
        "@app.route(\"/\", methods=['GET', 'POST'])\n",
        "def index():\n",
        "    global server_busy\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        user_input = request.form.get('text_input')\n",
        "        # Process the input as needed\n",
        "        print('received request for', user_input)\n",
        "\n",
        "        # Check if the server is already busy\n",
        "        if server_busy:\n",
        "            return \"The server is busy! Please try again soon.\"\n",
        "\n",
        "        # Set the server state as busy\n",
        "        server_busy = True\n",
        "\n",
        "        try:\n",
        "            generateReadmeModel(user_input)\n",
        "            return redirect(\"https://github.com/gabrielramp/hackgtsecret\")\n",
        "            server_busy = False\n",
        "        finally:\n",
        "            # Reset the server state back to not busy\n",
        "            server_busy = False\n",
        "\n",
        "    return '''\n",
        "        <form method=\"post\" action=\"/\">\n",
        "            <input type=\"text\" name=\"text_input\">\n",
        "            <input type=\"submit\" value=\"Enter\">\n",
        "        </form>\n",
        "    '''\n",
        "@app.route(\"/webhook\", methods=[\"POST\"])\n",
        "def handle_github_webhook():\n",
        "    global server_busy\n",
        "\n",
        "    # Verify if the incoming webhook is from GitHub\n",
        "    signature = request.headers.get(\"X-Hub-Signature\")\n",
        "    if not signature or not is_valid_signature(request.data, signature):\n",
        "        return jsonify({\"message\": \"Invalid request\"}), 400\n",
        "\n",
        "    # Get the payload data\n",
        "    data = request.json\n",
        "\n",
        "    # Check if the event is an issue and it's opened\n",
        "    if request.headers.get(\"X-GitHub-Event\") == \"issues\" and data[\"action\"] == \"opened\":\n",
        "        issue_title = data[\"issue\"][\"title\"]\n",
        "        issue_number = data[\"issue\"][\"number\"]\n",
        "        reply_to_issue(issue_number, \"Request received -- Please wait!\")\n",
        "\n",
        "        # Check if the server is already busy\n",
        "        if server_busy:\n",
        "            reply_to_issue(issue_number, \"The server is currently busy. Please try again soon!\")\n",
        "            close_issue(issue_number)\n",
        "            return jsonify({\"message\": \"The server is busy! Please try again soon.\"}), 503\n",
        "\n",
        "        # Set the server state as busy\n",
        "        server_busy = True\n",
        "\n",
        "        try:\n",
        "            generateReadmeModel(issue_title)\n",
        "            reply_to_issue(issue_number, \"README.md updated, closing ticket!\")\n",
        "            close_issue(issue_number)\n",
        "            return jsonify({\"message\": \"Processed successfully!\"}), 200\n",
        "        finally:\n",
        "            # Reset the server state back to not busy\n",
        "            server_busy = False\n",
        "\n",
        "\n",
        "# Start the Flask server in a new thread\n",
        "threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()\n",
        "\n",
        "#DEBUG force generation on cell start\n",
        "#generateReadmeModel(\"a spaceship\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9 (main, Aug 15 2022, 16:40:41) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b270b0f43bc427bcab7703c037711644cc480aac7c1cc8d2940cfaf0b447ee2e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}